{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibmm-unibe-ch/msa-tests/blob/master/FrankenFold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FrankenMSA\n",
        "## [Deep learning protein folding models predict alternative protein conformations with informative sequence alignments](https://github.com/ibmm-unibe-ch/msa-tests)"
      ],
      "metadata": {
        "id": "1eiy-dduz7LR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, fill out the first form before clicking play.\n",
        "You can select \"Upload none\" to upload nothing and not consider it at all. We allow for PDB structures directly from the PDB using it's PDB identifier.\n",
        "\n",
        "Currently, we only support monomers and inverse fold chain \"A\"."
      ],
      "metadata": {
        "id": "vyqQHDjh0ckf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interactive, VBox, HBox, Text, Button\n",
        "from IPython.display import display\n",
        "import functools\n",
        "import requests\n",
        "import hashlib\n",
        "import tarfile\n",
        "import time\n",
        "import os\n",
        "from typing import Tuple, List\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "from string import ascii_uppercase,ascii_lowercase\n",
        "from pathlib import Path\n",
        "import math\n",
        "from google.colab import files\n",
        "import uuid\n",
        "\n",
        "#@markdown #Input Options\n",
        "Query_sequence = '' #@param {type:\"string\"}\n",
        "MSA_depth = 128 #@param {type:\"integer\"}\n",
        "\n",
        "def get_pdb(pdb_code=\"\"):\n",
        "  if pdb_code == \"Upload none\":\n",
        "    return None\n",
        "  if pdb_code is None or pdb_code == \"Upload\":\n",
        "    upload_dict = files.upload()\n",
        "    pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
        "    filename = str(uuid.uuid4())+\".pdb\"\n",
        "    with open(filename,\"wb\") as out: out.write(pdb_string)\n",
        "    return filename\n",
        "  else:\n",
        "    os.system(f\"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb\")\n",
        "    return f\"{pdb_code}.pdb\"\n",
        "\n",
        "def get_msa(msa_code=\"\"):\n",
        "  if msa_code is None or msa_code == \"Upload none\":\n",
        "    upload_dict = files.upload()\n",
        "    pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
        "    filename = str(uuid.uuid4())+\".a3m\"\n",
        "    with open(filename,\"wb\") as out: out.write(pdb_string)\n",
        "    return filename\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "PDB_to_inverse_fold='Upload none' #@param {type:\"string\"}\n",
        "pdb_path = get_pdb(PDB_to_inverse_fold)\n",
        "\n",
        "Own_MSA='Upload none' #@param {type:\"string\"}\n",
        "msa_path = get_msa(Own_MSA)\n",
        "\n",
        "ProteinMPNN_model_name = \"v_48_020\" #@param [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\n",
        "model_name = ProteinMPNN_model_name\n",
        "\n",
        "# remove whitespaces\n",
        "query_sequence = \"\".join(Query_sequence.split())\n",
        "\n",
        "new_part_button = widgets.Button(description='Confirm')\n",
        "mmseqs_type_widget = widgets.Dropdown(options=['Full','Part',],value='Full',description='Query:',disabled=False)\n",
        "inverse_folding_options_widget = widgets.BoundedFloatText(value=1,min=0,max=10,step=0.1,description='Temperature',disabled=False)\n",
        "repeat_text_widget =widgets.Text(value='',placeholder='Empty == query',description='Repeated text:',disabled=False)\n",
        "box = None\n",
        "part_type = \"\"\n",
        "parts = []\n",
        "start = 0\n",
        "seq_len = len(query_sequence)\n",
        "print(f\"Protein has length: {seq_len}\")\n",
        "end = seq_len\n",
        "\n",
        "def set_box(box_object):\n",
        "  global box\n",
        "  box = box_object\n",
        "  return box_object\n",
        "def create_start_widget(end, start:int=0):\n",
        "  return widgets.BoundedIntText(value=start,min=0,max=end,step=1,description='Start',disabled=False)\n",
        "def create_end_widget(end, start:int=0):\n",
        "  return widgets.BoundedIntText(value=start,min=0,max=end,step=1,description='End',disabled=False)\n",
        "def get_mmseqs_widget(end:int, start:int=0):\n",
        "  return set_box(widgets.HBox([create_start_widget(end, start), create_end_widget(end, start), mmseqs_type_widget, new_part_button]))\n",
        "def get_inverse_folding_widget(end:int, start:int=0):\n",
        "  return set_box(widgets.HBox([create_start_widget(end, start), create_end_widget(end, start), inverse_folding_options_widget, new_part_button]))\n",
        "def get_repeat_widget(end:int, start:int=0):\n",
        "  return set_box(widgets.HBox([create_start_widget(end, start), create_end_widget(end, start), repeat_text_widget, new_part_button]))\n",
        "def get_gap_widget(end:int, start:int=0):\n",
        "  return set_box(widgets.HBox([create_start_widget(end, start), create_end_widget(end, start), new_part_button]))\n",
        "def get_own_msa_widget(end:int, start:int=0):\n",
        "  return set_box(widgets.HBox([create_start_widget(end, start), create_end_widget(end, start), new_part_button]))\n",
        "\n",
        "def on_new_button_clicked(*args):\n",
        "  global box, part_type, parts, start\n",
        "  if part_type == \"MMseqs\":\n",
        "    other = box.children[2].value\n",
        "    addendum = f\" query: {other}\"\n",
        "  elif part_type == \"Inverse folding\":\n",
        "    other = box.children[2].value\n",
        "    addendum = f\" temperature: {other}\"\n",
        "  elif part_type == \"Repeat\":\n",
        "    other = box.children[2].value if box.children[2].value not in [\"\", 'Empty == query'] else None\n",
        "    addendum = f\" sequence: {other}\" if other else \"\"\n",
        "  else:\n",
        "    other = None\n",
        "    addendum = \"\"\n",
        "  print(f\"[{box.children[0].value}:{box.children[1].value}], {part_type}{addendum}\")\n",
        "  parts.append((part_type,box.children[0].value, box.children[1].value, other))\n",
        "  start = box.children[1].value\n",
        "  return interactive(callback, Part_type=type_options)\n",
        "\n",
        "new_part_button.on_click(functools.partial(on_new_button_clicked))\n",
        "\n",
        "def callback(Part_type):\n",
        "  global part_type, start\n",
        "  part_type = Part_type\n",
        "  type_callbacks = {\"Gaps\": get_gap_widget, 'Inverse folding':get_inverse_folding_widget, 'MMseqs':get_mmseqs_widget, 'Own MSA': get_own_msa_widget, 'Repeat': get_repeat_widget}\n",
        "  outputt = type_callbacks[Part_type](seq_len,start)\n",
        "  display(outputt)\n",
        "type_options = ['Gaps', 'Inverse folding', 'MMseqs', 'Own MSA', 'Repeat']\n",
        "\n",
        "interactive(callback, Part_type=type_options)"
      ],
      "metadata": {
        "id": "m9cR7GoegLip",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download hhsuite\n",
        "%%capture\n",
        "!wget https://github.com/soedinglab/hh-suite/releases/download/v3.3.0/hhsuite-3.3.0-AVX2-Linux.tar.gz; tar xvfz hhsuite-3.3.0-AVX2-Linux.tar.gz; export PATH=\"$(pwd)/bin:$(pwd)/scripts:$PATH\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "s009HpUZjHJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ckAZuH-K0k",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/sokrypton/ColabFold/blob/main/colabfold/colabfold.py\n",
        "#@title MMSeqs\n",
        "\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "aatypes = set('ACDEFGHIKLMNPQRSTVWY')\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "def run_mmseqs2(x, prefix, use_env=True, use_filter=True,\n",
        "                use_templates=False, filter=None, use_pairing=False, pairing_strategy=\"greedy\",\n",
        "                host_url=\"https://api.colabfold.com\",\n",
        "                user_agent: str = \"\") -> Tuple[List[str], List[str]]:\n",
        "  submission_endpoint = \"ticket/pair\" if use_pairing else \"ticket/msa\"\n",
        "  headers = {}\n",
        "  if user_agent != \"\":\n",
        "    headers['User-Agent'] = user_agent\n",
        "  else:\n",
        "    logger.warning(\"No user agent specified. Please set a user agent (e.g., 'toolname/version contact@email') to help us debug in case of problems. This warning will become an error in the future.\")\n",
        "\n",
        "  def submit(seqs, mode, N=101):\n",
        "    n, query = N, \"\"\n",
        "    for seq in seqs:\n",
        "      query += f\">{n}\\n{seq}\\n\"\n",
        "      n += 1\n",
        "\n",
        "    while True:\n",
        "      error_count = 0\n",
        "      try:\n",
        "        # https://requests.readthedocs.io/en/latest/user/advanced/#advanced\n",
        "        # \"good practice to set connect timeouts to slightly larger than a multiple of 3\"\n",
        "        res = requests.post(f'{host_url}/{submission_endpoint}', data={ 'q': query, 'mode': mode }, timeout=6.02, headers=headers)\n",
        "      except requests.exceptions.Timeout:\n",
        "        logger.warning(\"Timeout while submitting to MSA server. Retrying...\")\n",
        "        continue\n",
        "      except Exception as e:\n",
        "        error_count += 1\n",
        "        logger.warning(f\"Error while fetching result from MSA server. Retrying... ({error_count}/5)\")\n",
        "        logger.warning(f\"Error: {e}\")\n",
        "        time.sleep(5)\n",
        "        if error_count > 5:\n",
        "          raise\n",
        "        continue\n",
        "      break\n",
        "\n",
        "    try:\n",
        "      out = res.json()\n",
        "    except ValueError:\n",
        "      logger.error(f\"Server didn't reply with json: {res.text}\")\n",
        "      out = {\"status\":\"ERROR\"}\n",
        "    return out\n",
        "\n",
        "  def status(ID):\n",
        "    while True:\n",
        "      error_count = 0\n",
        "      try:\n",
        "        res = requests.get(f'{host_url}/ticket/{ID}', timeout=6.02, headers=headers)\n",
        "      except requests.exceptions.Timeout:\n",
        "        logger.warning(\"Timeout while fetching status from MSA server. Retrying...\")\n",
        "        continue\n",
        "      except Exception as e:\n",
        "        error_count += 1\n",
        "        logger.warning(f\"Error while fetching result from MSA server. Retrying... ({error_count}/5)\")\n",
        "        logger.warning(f\"Error: {e}\")\n",
        "        time.sleep(5)\n",
        "        if error_count > 5:\n",
        "          raise\n",
        "        continue\n",
        "      break\n",
        "    try:\n",
        "      out = res.json()\n",
        "    except ValueError:\n",
        "      logger.error(f\"Server didn't reply with json: {res.text}\")\n",
        "      out = {\"status\":\"ERROR\"}\n",
        "    return out\n",
        "\n",
        "  def download(ID, path):\n",
        "    error_count = 0\n",
        "    while True:\n",
        "      try:\n",
        "        res = requests.get(f'{host_url}/result/download/{ID}', timeout=6.02, headers=headers)\n",
        "      except requests.exceptions.Timeout:\n",
        "        logger.warning(\"Timeout while fetching result from MSA server. Retrying...\")\n",
        "        continue\n",
        "      except Exception as e:\n",
        "        error_count += 1\n",
        "        logger.warning(f\"Error while fetching result from MSA server. Retrying... ({error_count}/5)\")\n",
        "        logger.warning(f\"Error: {e}\")\n",
        "        time.sleep(5)\n",
        "        if error_count > 5:\n",
        "          raise\n",
        "        continue\n",
        "      break\n",
        "    with open(path,\"wb\") as out: out.write(res.content)\n",
        "\n",
        "  # process input x\n",
        "  seqs = [x] if isinstance(x, str) else x\n",
        "\n",
        "  # compatibility to old option\n",
        "  if filter is not None:\n",
        "    use_filter = filter\n",
        "\n",
        "  # setup mode\n",
        "  if use_filter:\n",
        "    mode = \"env\" if use_env else \"all\"\n",
        "  else:\n",
        "    mode = \"env-nofilter\" if use_env else \"nofilter\"\n",
        "\n",
        "  if use_pairing:\n",
        "    use_templates = False\n",
        "    mode = \"\"\n",
        "    # greedy is default, complete was the previous behavior\n",
        "    if pairing_strategy == \"greedy\":\n",
        "      mode = \"pairgreedy\"\n",
        "    elif pairing_strategy == \"complete\":\n",
        "      mode = \"paircomplete\"\n",
        "    if use_env:\n",
        "      mode = mode + \"-env\"\n",
        "\n",
        "  # define path\n",
        "  path = f\"{prefix}_{mode}\"\n",
        "  if not os.path.isdir(path): os.mkdir(path)\n",
        "\n",
        "  # call mmseqs2 api\n",
        "  tar_gz_file = f'{path}/out.tar.gz'\n",
        "  N,REDO = 101,True\n",
        "\n",
        "  # deduplicate and keep track of order\n",
        "  seqs_unique = []\n",
        "  #TODO this might be slow for large sets\n",
        "  [seqs_unique.append(x) for x in seqs if x not in seqs_unique]\n",
        "  Ms = [N + seqs_unique.index(seq) for seq in seqs]\n",
        "  # lets do it!\n",
        "  if not os.path.isfile(tar_gz_file):\n",
        "    TIME_ESTIMATE = 150 * len(seqs_unique)\n",
        "    with tqdm(total=TIME_ESTIMATE, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "      while REDO:\n",
        "        pbar.set_description(\"SUBMIT\")\n",
        "\n",
        "        # Resubmit job until it goes through\n",
        "        out = submit(seqs_unique, mode, N)\n",
        "        while out[\"status\"] in [\"UNKNOWN\", \"RATELIMIT\"]:\n",
        "          sleep_time = 5 + random.randint(0, 5)\n",
        "          logger.error(f\"Sleeping for {sleep_time}s. Reason: {out['status']}\")\n",
        "          # resubmit\n",
        "          time.sleep(sleep_time)\n",
        "          out = submit(seqs_unique, mode, N)\n",
        "\n",
        "        if out[\"status\"] == \"ERROR\":\n",
        "          raise Exception(f'MMseqs2 API is giving errors. Please confirm your input is a valid protein sequence. If error persists, please try again an hour later.')\n",
        "\n",
        "        if out[\"status\"] == \"MAINTENANCE\":\n",
        "          raise Exception(f'MMseqs2 API is undergoing maintenance. Please try again in a few minutes.')\n",
        "\n",
        "        # wait for job to finish\n",
        "        ID,TIME = out[\"id\"],0\n",
        "        pbar.set_description(out[\"status\"])\n",
        "        while out[\"status\"] in [\"UNKNOWN\",\"RUNNING\",\"PENDING\"]:\n",
        "          t = 5 + random.randint(0,5)\n",
        "          logger.error(f\"Sleeping for {t}s. Reason: {out['status']}\")\n",
        "          time.sleep(t)\n",
        "          out = status(ID)\n",
        "          pbar.set_description(out[\"status\"])\n",
        "          if out[\"status\"] == \"RUNNING\":\n",
        "            TIME += t\n",
        "            pbar.update(n=t)\n",
        "          #if TIME > 900 and out[\"status\"] != \"COMPLETE\":\n",
        "          #  # something failed on the server side, need to resubmit\n",
        "          #  N += 1\n",
        "          #  break\n",
        "\n",
        "        if out[\"status\"] == \"COMPLETE\":\n",
        "          if TIME < TIME_ESTIMATE:\n",
        "            pbar.update(n=(TIME_ESTIMATE-TIME))\n",
        "          REDO = False\n",
        "\n",
        "        if out[\"status\"] == \"ERROR\":\n",
        "          REDO = False\n",
        "          raise Exception(f'MMseqs2 API is giving errors. Please confirm your input is a valid protein sequence. If error persists, please try again an hour later.')\n",
        "\n",
        "      # Download results\n",
        "      download(ID, tar_gz_file)\n",
        "\n",
        "  # prep list of a3m files\n",
        "  if use_pairing:\n",
        "    a3m_files = [f\"{path}/pair.a3m\"]\n",
        "  else:\n",
        "    a3m_files = [f\"{path}/uniref.a3m\"]\n",
        "    if use_env: a3m_files.append(f\"{path}/bfd.mgnify30.metaeuk30.smag30.a3m\")\n",
        "\n",
        "  # extract a3m files\n",
        "  if any(not os.path.isfile(a3m_file) for a3m_file in a3m_files):\n",
        "    with tarfile.open(tar_gz_file) as tar_gz:\n",
        "      tar_gz.extractall(path)\n",
        "\n",
        "  # templates\n",
        "  if use_templates:\n",
        "    templates = {}\n",
        "    #print(\"seq\\tpdb\\tcid\\tevalue\")\n",
        "    for line in open(f\"{path}/pdb70.m8\",\"r\"):\n",
        "      p = line.rstrip().split()\n",
        "      M,pdb,qid,e_value = p[0],p[1],p[2],p[10]\n",
        "      M = int(M)\n",
        "      if M not in templates: templates[M] = []\n",
        "      templates[M].append(pdb)\n",
        "      #if len(templates[M]) <= 20:\n",
        "      #  print(f\"{int(M)-N}\\t{pdb}\\t{qid}\\t{e_value}\")\n",
        "\n",
        "    template_paths = {}\n",
        "    for k,TMPL in templates.items():\n",
        "      TMPL_PATH = f\"{prefix}_{mode}/templates_{k}\"\n",
        "      if not os.path.isdir(TMPL_PATH):\n",
        "        os.mkdir(TMPL_PATH)\n",
        "        TMPL_LINE = \",\".join(TMPL[:20])\n",
        "        response = None\n",
        "        while True:\n",
        "          error_count = 0\n",
        "          try:\n",
        "            # https://requests.readthedocs.io/en/latest/user/advanced/#advanced\n",
        "            # \"good practice to set connect timeouts to slightly larger than a multiple of 3\"\n",
        "            response = requests.get(f\"{host_url}/template/{TMPL_LINE}\", stream=True, timeout=6.02, headers=headers)\n",
        "          except requests.exceptions.Timeout:\n",
        "            logger.warning(\"Timeout while submitting to template server. Retrying...\")\n",
        "            continue\n",
        "          except Exception as e:\n",
        "            error_count += 1\n",
        "            logger.warning(f\"Error while fetching result from template server. Retrying... ({error_count}/5)\")\n",
        "            logger.warning(f\"Error: {e}\")\n",
        "            time.sleep(5)\n",
        "            if error_count > 5:\n",
        "              raise\n",
        "            continue\n",
        "          break\n",
        "        with tarfile.open(fileobj=response.raw, mode=\"r|gz\") as tar:\n",
        "          tar.extractall(path=TMPL_PATH)\n",
        "        os.symlink(\"pdb70_a3m.ffindex\", f\"{TMPL_PATH}/pdb70_cs219.ffindex\")\n",
        "        with open(f\"{TMPL_PATH}/pdb70_cs219.ffdata\", \"w\") as f:\n",
        "          f.write(\"\")\n",
        "      template_paths[k] = TMPL_PATH\n",
        "\n",
        "  # gather a3m lines\n",
        "  a3m_lines = {}\n",
        "  for a3m_file in a3m_files:\n",
        "    update_M,M = True,None\n",
        "    for line in open(a3m_file,\"r\"):\n",
        "      if len(line) > 0:\n",
        "        if \"\\x00\" in line:\n",
        "          line = line.replace(\"\\x00\",\"\")\n",
        "          update_M = True\n",
        "        if line.startswith(\">\") and update_M:\n",
        "          M = int(line[1:].rstrip())\n",
        "          update_M = False\n",
        "          if M not in a3m_lines: a3m_lines[M] = []\n",
        "        a3m_lines[M].append(line)\n",
        "\n",
        "  # return results\n",
        "\n",
        "  a3m_lines = [\"\".join(a3m_lines[n]) for n in Ms]\n",
        "\n",
        "  if use_templates:\n",
        "    template_paths_ = []\n",
        "    for n in Ms:\n",
        "      if n not in template_paths:\n",
        "        template_paths_.append(None)\n",
        "        #print(f\"{n-N}\\tno_templates_found\")\n",
        "      else:\n",
        "        template_paths_.append(template_paths[n])\n",
        "    template_paths = template_paths_\n",
        "\n",
        "\n",
        "  return (a3m_lines, template_paths) if use_templates else a3m_lines\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# utils\n",
        "#########################################################################\n",
        "def get_hash(x):\n",
        "  return hashlib.sha1(x.encode()).hexdigest()\n",
        "\n",
        "def homooligomerize(msas, deletion_matrices, homooligomer=1):\n",
        " if homooligomer == 1:\n",
        "  return msas, deletion_matrices\n",
        " else:\n",
        "  new_msas = []\n",
        "  new_mtxs = []\n",
        "  for o in range(homooligomer):\n",
        "    for msa,mtx in zip(msas, deletion_matrices):\n",
        "      num_res = len(msa[0])\n",
        "      L = num_res * o\n",
        "      R = num_res * (homooligomer-(o+1))\n",
        "      new_msas.append([\"-\"*L+s+\"-\"*R for s in msa])\n",
        "      new_mtxs.append([[0]*L+m+[0]*R for m in mtx])\n",
        "  return new_msas, new_mtxs\n",
        "\n",
        "# keeping typo for cross-compatibility\n",
        "def homooliomerize(msas, deletion_matrices, homooligomer=1):\n",
        "  return homooligomerize(msas, deletion_matrices, homooligomer=homooligomer)\n",
        "\n",
        "def homooligomerize_heterooligomer(msas, deletion_matrices, lengths, homooligomers):\n",
        "  '''\n",
        "  ----- inputs -----\n",
        "  msas: list of msas\n",
        "  deletion_matrices: list of deletion matrices\n",
        "  lengths: list of lengths for each component in complex\n",
        "  homooligomers: list of number of homooligomeric copies for each component\n",
        "  ----- outputs -----\n",
        "  (msas, deletion_matrices)\n",
        "  '''\n",
        "  if max(homooligomers) == 1:\n",
        "    return msas, deletion_matrices\n",
        "\n",
        "  elif len(homooligomers) == 1:\n",
        "    return homooligomerize(msas, deletion_matrices, homooligomers[0])\n",
        "\n",
        "  else:\n",
        "    frag_ij = [[0,lengths[0]]]\n",
        "    for length in lengths[1:]:\n",
        "      j = frag_ij[-1][-1]\n",
        "      frag_ij.append([j,j+length])\n",
        "\n",
        "    # for every msa\n",
        "    mod_msas, mod_mtxs = [],[]\n",
        "    for msa, mtx in zip(msas, deletion_matrices):\n",
        "      mod_msa, mod_mtx = [],[]\n",
        "      # for every sequence\n",
        "      for n,(s,m) in enumerate(zip(msa,mtx)):\n",
        "        # split sequence\n",
        "        _s,_m,_ok = [],[],[]\n",
        "        for i,j in frag_ij:\n",
        "          _s.append(s[i:j]); _m.append(m[i:j])\n",
        "          _ok.append(max([o != \"-\" for o in _s[-1]]))\n",
        "\n",
        "        if n == 0:\n",
        "          # if first query sequence\n",
        "          mod_msa.append(\"\".join([x*h for x,h in zip(_s,homooligomers)]))\n",
        "          mod_mtx.append(sum([x*h for x,h in zip(_m,homooligomers)],[]))\n",
        "\n",
        "        elif sum(_ok) == 1:\n",
        "          # elif one fragment: copy each fragment to every homooligomeric copy\n",
        "          a = _ok.index(True)\n",
        "          for h_a in range(homooligomers[a]):\n",
        "            _blank_seq = [[\"-\"*l]*h for l,h in zip(lengths,homooligomers)]\n",
        "            _blank_mtx = [[[0]*l]*h for l,h in zip(lengths,homooligomers)]\n",
        "            _blank_seq[a][h_a] = _s[a]\n",
        "            _blank_mtx[a][h_a] = _m[a]\n",
        "            mod_msa.append(\"\".join([\"\".join(x) for x in _blank_seq]))\n",
        "            mod_mtx.append(sum([sum(x,[]) for x in _blank_mtx],[]))\n",
        "        else:\n",
        "          # else: copy fragment pair to every homooligomeric copy pair\n",
        "          for a in range(len(lengths)-1):\n",
        "            if _ok[a]:\n",
        "              for b in range(a+1,len(lengths)):\n",
        "                if _ok[b]:\n",
        "                  for h_a in range(homooligomers[a]):\n",
        "                    for h_b in range(homooligomers[b]):\n",
        "                      _blank_seq = [[\"-\"*l]*h for l,h in zip(lengths,homooligomers)]\n",
        "                      _blank_mtx = [[[0]*l]*h for l,h in zip(lengths,homooligomers)]\n",
        "                      for c,h_c in zip([a,b],[h_a,h_b]):\n",
        "                        _blank_seq[c][h_c] = _s[c]\n",
        "                        _blank_mtx[c][h_c] = _m[c]\n",
        "                      mod_msa.append(\"\".join([\"\".join(x) for x in _blank_seq]))\n",
        "                      mod_mtx.append(sum([sum(x,[]) for x in _blank_mtx],[]))\n",
        "      mod_msas.append(mod_msa)\n",
        "      mod_mtxs.append(mod_mtx)\n",
        "    return mod_msas, mod_mtxs\n",
        "\n",
        "def chain_break(idx_res, Ls, length=200):\n",
        "  # Minkyung's code\n",
        "  # add big enough number to residue index to indicate chain breaks\n",
        "  L_prev = 0\n",
        "  for L_i in Ls[:-1]:\n",
        "    idx_res[L_prev+L_i:] += length\n",
        "    L_prev += L_i\n",
        "  return idx_res\n",
        "\n",
        "def read_pdb_renum(pdb_filename, Ls=None):\n",
        "  if Ls is not None:\n",
        "    L_init = 0\n",
        "    new_chain = {}\n",
        "    for L,c in zip(Ls, alphabet_list):\n",
        "      new_chain.update({i:c for i in range(L_init,L_init+L)})\n",
        "      L_init += L\n",
        "\n",
        "  n,pdb_out = 1,[]\n",
        "  resnum_,chain_ = 1,\"A\"\n",
        "  for line in open(pdb_filename,\"r\"):\n",
        "    if line[:4] == \"ATOM\":\n",
        "      chain = line[21:22]\n",
        "      resnum = int(line[22:22+5])\n",
        "      if resnum != resnum_ or chain != chain_:\n",
        "        resnum_,chain_ = resnum,chain\n",
        "        n += 1\n",
        "      if Ls is None: pdb_out.append(\"%s%4i%s\" % (line[:22],n,line[26:]))\n",
        "      else: pdb_out.append(\"%s%s%4i%s\" % (line[:21],new_chain[n-1],n,line[26:]))\n",
        "  return \"\".join(pdb_out)\n",
        "\n",
        "\n",
        "def kabsch(a, b, weights=None, return_v=False):\n",
        "  a = np.asarray(a,float)\n",
        "  b = np.asarray(b,float)\n",
        "  if weights is None: weights = np.ones(len(b))\n",
        "  else: weights = np.asarray(weights,float)\n",
        "  B = np.einsum('ji,jk->ik', weights[:, None] * a, b)\n",
        "  u, s, vh = np.linalg.svd(B)\n",
        "  if np.linalg.det(u @ vh) < 0: u[:, -1] = -u[:, -1]\n",
        "  if return_v: return u\n",
        "  else: return u @ vh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MMseqs clean-up\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def get_mmseqs_from_string(seq:str):\n",
        "  lines = run_mmseqs2(seq, \"FrankenMSA\", user_agent=\"FrankenMSA/jannik.gut@unibe.ch\")\n",
        "  lines = lines[0].split(\"\\n\")\n",
        "  return lines\n",
        "\n",
        "def dump_msa_to_file(msa:List[str], path:Path):\n",
        "  with open(path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(msa))\n",
        "\n",
        "def read_msa_from_file(path:Path):\n",
        "  with open(path, \"r\") as f:\n",
        "    return [line.strip() for line in f.read().split(\"\\n\") if len(line.strip())>0]\n",
        "\n",
        "def sequences_from_msa(msa:List[str]):\n",
        "  return [line for line in msa if not line.startswith(\">\")]\n",
        "\n",
        "def process_msa(msa:List[str], depth:int=128):\n",
        "  if isinstance(msa, Path):\n",
        "    msa = read_msa_from_file(msa)\n",
        "  length = len(msa[1])\n",
        "  dump_msa_to_file(msa, Path(\"msa_input.a3m\"))\n",
        "  input_path = \"msa_input.a3m\"\n",
        "  formated_path = \"msa_formated.a3m\"\n",
        "  cut_path = \"msa_cut.a3m\"\n",
        "  output_path = \"msa_output.a3m\"\n",
        "  sh = f\"\"\"\n",
        "perl /content/scripts/reformat.pl -r -l {length} {input_path} {formated_path};\n",
        "/content/bin/hhfilter -M first -diff {depth} -i {formated_path} -o {cut_path};\n",
        "head -n {2*depth} {cut_path}>{output_path}\n",
        "\"\"\"\n",
        "  with open('process_msa_script.sh', 'w') as file:\n",
        "    file.write(sh)\n",
        "  !bash process_msa_script.sh &> out\n",
        "  return read_msa_from_file(output_path)\n",
        "\n",
        "def run_mmseqs_pipeline(seq:str, depth:int=128):\n",
        "  temp_msa = get_mmseqs_from_string(seq)\n",
        "  msa = process_msa(temp_msa, depth=depth)\n",
        "  if len(msa)<(2*depth):\n",
        "    multiplied_msa = msa*int((2*depth)/len(msa))\n",
        "    if ((2*depth)%len(msa)) !=0:\n",
        "      added_msa = msa[:(1+(2*depth)%len(msa))]\n",
        "  out_msa =multiplied_msa+added_msa\n",
        "  return out_msa"
      ],
      "metadata": {
        "id": "TwxvWRUFJpaR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simpler operations\n",
        "\n",
        "def concat_horizontally(msa1:List[str], msa2:List[str]):\n",
        "  for i in range(len(msa1)):\n",
        "    msa1[i] += msa2[i]\n",
        "  return msa1\n",
        "\n",
        "def select_horizontally(msa:List[str], start:int, end:int):\n",
        "  return_msa = [\"\"]*len(msa)\n",
        "  for i in range(len(msa)):\n",
        "    return_msa[i] = msa[i][start:end]\n",
        "  return return_msa\n",
        "\n",
        "def repeat_until(msa:List[str], depth:int=128) -> List[str]:\n",
        "  num_repetitions = int(np.ceil(depth/(len(msa)/2)))\n",
        "  too_big = msa*num_repetitions\n",
        "  return too_big[:(2*depth)]\n",
        "\n",
        "def fill_gaps_until(msa:List[str], depth:int=128) -> List[str]:\n",
        "  seq_len = len(msa[1])\n",
        "  gap_seq = [\">gap_seq\", '-'*seq_len]\n",
        "  len_msa = len(msa)/2\n",
        "  add = int(math.ceil(depth-len_msa))\n",
        "  msa = msa + gap_seq*add\n",
        "  return msa"
      ],
      "metadata": {
        "id": "NQycrJpoYJaI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download ProteinMPNN\n",
        "import json, time, os, sys, glob\n",
        "\n",
        "if not os.path.isdir(\"ProteinMPNN\"):\n",
        "  os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "sys.path.append('/content/ProteinMPNN')"
      ],
      "metadata": {
        "id": "AuH0ctF9XW2B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ProteinMPNN\n",
        "import torch\n",
        "import copy\n",
        "from protein_mpnn_utils import tied_featurize, parse_PDB, ProteinMPNN, StructureDataset, StructureDatasetPDB, _scores, _S_to_seq\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "def load_model():\n",
        "  device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "  model_name = \"v_48_020\"\n",
        "  backbone_noise=0.00\n",
        "  hidden_dim = 128\n",
        "  num_layers = 3\n",
        "  model_folder_path = '/content/ProteinMPNN/vanilla_model_weights'\n",
        "  if model_folder_path[-1] != '/':\n",
        "      model_folder_path = model_folder_path + '/'\n",
        "  checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
        "  checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "  noise_level_print = checkpoint['noise_level']\n",
        "  model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
        "  model.to(device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "def make_tied_positions_for_homomers(pdb_dict_list):\n",
        "    my_dict = {}\n",
        "    for result in pdb_dict_list:\n",
        "        all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n",
        "        tied_positions_list = []\n",
        "        chain_length = len(result[f\"seq_chain_{all_chain_list[0]}\"])\n",
        "        for i in range(1,chain_length+1):\n",
        "            temp_dict = {}\n",
        "            for j, chain in enumerate(all_chain_list):\n",
        "                temp_dict[chain] = [i] #needs to be a list\n",
        "            tied_positions_list.append(temp_dict)\n",
        "        my_dict[result['name']] = tied_positions_list\n",
        "    return my_dict\n",
        "\n",
        "## proteinMPNN params\n",
        "max_length=20000\n",
        "BATCH_COPIES = 1 #batch_size\n",
        "pssm_threshold = 0\n",
        "pssm_multi = 0\n",
        "pssm_log_odds_flag =0\n",
        "pssm_bias_flag = 0\n",
        "\n",
        "def generate_proteinMPNN(input_path:Path, input_chain_list:List[str]=[\"A\"], sampling_temp:float=1.0, num_seqs:int=128):\n",
        "  NUM_BATCHES = num_seqs//BATCH_COPIES\n",
        "  with torch.no_grad():\n",
        "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "    bias_AAs_np = np.zeros(len(alphabet))\n",
        "    omit_AAs_np = np.array([AA in \"X\" for AA in alphabet]).astype(np.float32)\n",
        "    model = load_model()\n",
        "    pdb_dict_list = parse_PDB(input_path, input_chain_list=input_chain_list)\n",
        "    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
        "    chain_id_dict = {}\n",
        "    #fixed_chain_list = all_chains-input_chain_list\n",
        "    chain_id_dict[pdb_dict_list[0]['name']]= (input_chain_list,[]) #fixed_chain_list)\n",
        "    for ix, protein in enumerate(dataset_valid):\n",
        "      batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
        "      tied_positions_dict = None#make_tied_positions_for_homomers([input_path]) if homomer else None\n",
        "      X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, next(model.parameters()).device, chain_id_dict, None, None, tied_positions_dict, None, None)\n",
        "      pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n",
        "      name_ = batch_clones[0]['name']\n",
        "      randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "      log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
        "      mask_for_loss = mask*chain_M*chain_M_pos\n",
        "      lines = []\n",
        "      for j in range(NUM_BATCHES):\n",
        "          randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
        "          if tied_positions_dict == None:\n",
        "              sample_dict = model.sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=sampling_temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
        "              S_sample = sample_dict[\"S\"]\n",
        "          else:\n",
        "              sample_dict = model.tied_sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=sampling_temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta, bias_by_res=bias_by_res_all)\n",
        "          # Compute scores\n",
        "              S_sample = sample_dict[\"S\"]\n",
        "          log_probs = model(X, S_sample, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_2, use_input_decoding_order=True, decoding_order=sample_dict[\"decoding_order\"])\n",
        "          mask_for_loss = mask*chain_M*chain_M_pos\n",
        "          scores = _scores(S_sample, log_probs, mask_for_loss)\n",
        "          for b_ix in range(BATCH_COPIES):\n",
        "              masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
        "              masked_list = masked_list_list[b_ix]\n",
        "              seq_recovery_rate = torch.sum(torch.sum(torch.nn.functional.one_hot(S[b_ix], 21)*torch.nn.functional.one_hot(S_sample[b_ix], 21),axis=-1)*mask_for_loss[b_ix])/torch.sum(mask_for_loss[b_ix])\n",
        "              seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
        "              score = scores[b_ix]\n",
        "              native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
        "              if b_ix == 0 and j==0:\n",
        "                  start = 0\n",
        "                  end = 0\n",
        "                  list_of_AAs = []\n",
        "                  for mask_l in masked_chain_length_list:\n",
        "                      end += mask_l\n",
        "                      list_of_AAs.append(native_seq[start:end])\n",
        "                      start = end\n",
        "                  native_seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
        "                  l0 = 0\n",
        "                  for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
        "                      l0 += mc_length\n",
        "                      native_seq = native_seq[:l0] + '/' + native_seq[l0:]\n",
        "                      l0 += 1\n",
        "                  sorted_masked_chain_letters = np.argsort(masked_list_list[0])\n",
        "                  print_masked_chains = [masked_list_list[0][i] for i in sorted_masked_chain_letters]\n",
        "                  sorted_visible_chain_letters = np.argsort(visible_list_list[0])\n",
        "                  print_visible_chains = [visible_list_list[0][i] for i in sorted_visible_chain_letters]\n",
        "                  line = '>{}, fixed_chains={}, designed_chains={}, model_name={}\\n{}\\n'.format(name_, print_visible_chains, print_masked_chains, model_name, native_seq)\n",
        "              start = 0\n",
        "              end = 0\n",
        "              list_of_AAs = []\n",
        "              for mask_l in masked_chain_length_list:\n",
        "                  end += mask_l\n",
        "                  list_of_AAs.append(seq[start:end])\n",
        "                  start = end\n",
        "              seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
        "              l0 = 0\n",
        "              for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
        "                  l0 += mc_length\n",
        "                  seq = seq[:l0] + '/' + seq[l0:]\n",
        "                  l0 += 1\n",
        "              seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
        "              line = '>T={}, sample={}, seq_recovery={}\\n{}\\n'.format(sampling_temp,b_ix,seq_rec_print,seq)\n",
        "              lines.append(line.rstrip())\n",
        "      out = []\n",
        "      for line in lines:\n",
        "        out += line.split(\"\\n\")\n",
        "      return out"
      ],
      "metadata": {
        "id": "Fj6pSuopbqXO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate things\n",
        "\n",
        "full_mmseqs = None\n",
        "full_inverse_folding = {}\n",
        "output_msa = [\"\"]*MSA_depth\n",
        "prepared_own_msa = None\n",
        "for (part_type,start,end, other) in parts:\n",
        "  if part_type == \"Gaps\":\n",
        "    curr_msa = fill_gaps_until([None,query_sequence[start:end]], MSA_depth)[1::2]\n",
        "  elif part_type == \"Repeat\":\n",
        "    if other is None:\n",
        "      curr_msa = repeat_until([None, query_sequence[start:end]], MSA_depth)[1::2]\n",
        "    else:\n",
        "      curr_msa = repeat_until([other], MSA_depth-1)[1::2]\n",
        "      curr_msa = [query_sequence[start:end]]+curr_msa\n",
        "  elif part_type == \"MMseqs\":\n",
        "    if full_mmseqs is None and other == \"Full\":\n",
        "      full_mmseqs = run_mmseqs_pipeline(query_sequence, MSA_depth)\n",
        "    if other == \"Full\":\n",
        "      curr_msa = select_horizontally(full_mmseqs, start, end)[1::2]\n",
        "    else:\n",
        "      curr_msa =run_mmseqs_pipeline(query_sequence[start:end], MSA_depth)[1::2]\n",
        "  elif part_type == \"Inverse folding\":\n",
        "    if full_inverse_folding.get(other, None) is None:\n",
        "      full_inverse_folding[other] = generate_proteinMPNN(pdb_path, [\"A\"], other, MSA_depth)\n",
        "    curr_msa = select_horizontally(full_inverse_folding[other], start, end)[1::2]\n",
        "  elif part_type == \"Own MSA\":\n",
        "    if prepared_own_msa is None:\n",
        "      prepared_own_msa = process_msa(Path(msa_path))\n",
        "    curr_msa = select_horizontally(prepared_own_msa, start, end)[1::2]\n",
        "  else:\n",
        "    print(f\"Unknown part type {part_type}\")\n",
        "  output_msa = concat_horizontally(output_msa, curr_msa)\n",
        "output_msa"
      ],
      "metadata": {
        "id": "w9DIwskP0j8D",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}